# 2.2 コードを実行してテキスト生成を行う

このセクションでは、Bedrockを使ってテキスト生成を行います。

以下の内容で「simple_chat.py」という名前で新規ファイルを作成します。

```python:simple_chat.py
import os
from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


model_id = 'anthropic.claude-3-haiku-20240307-v1:0'
region_name = 'us-east-1'


chat = ChatBedrock(
    model_id=model_id,
    model_kwargs={"temperature": 0.1},
    region_name=region_name
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたはAIアシスタントです。"),
        MessagesPlaceholder(variable_name="messages"),
    ]
)


def chat_simple(question):
    chain = prompt | chat
    result = chain.invoke(
        {
            "messages": [HumanMessage(content=question)]
        }
    )
    return result.content


if __name__ == "__main__":
    question = "日本の首都はどこですか？"
    print(chat_simple(question))
```

## コードの解説

このコードでは、LangChain から Bedrock のモデル呼び出しを行っています。

```python:モデルの呼び出し
chat = ChatBedrock(
    model_id=model_id,
    model_kwargs={"temperature": 0.1},
    region_name=region_name
)
```
上記のように [ChatBedrock クラス](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/)へ利用するモデルID(model_id)、モデルへのパラメータ(model_kwargs)、利用するモデルのAWSリージョン(region_name)を指定します。

- [モデルIDについて](https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/model-ids.html)
- [モデルパラメータ](https://docs.aws.amazon.com/ja_jp/bedrock/latest/userguide/model-parameters.html)

今回のコードでは、 Anthropic 社の Claude 3 Haiku を利用します。


```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたはAIアシスタントです。"),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```
上記のコードでは、 [ChatPromptTemplate クラス](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) を使ってモデルへ入力するプロンプトの定義を行なっています。

この時点では、入力テキストは与えられておらず、[MessagesPlaceholder クラス](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) で入力テキストを受け取る際のプレースホルダーを定義しています。


```python
chain = prompt | chat
result = chain.invoke(
    {
        "messages": [HumanMessage(content=question)]
    }
)
return result.content
```
上記のコードにて、messagesプレスホルダーに入力テキストを追加します。

追加する入力テキストについては、ユーザ（人間）のメッセージとなるため、`HumanMessage(content=question)`のように指定します。

また、モデルの出力については、content属性から取得することが可能です。

## コードの実行

それでは、実際にコードの実行を行います。

```shell
python3 simple_chat.py
```

すると、テキスト生成が行われ、以下のように出力されます。

**実行結果(例)**
```text
日本の首都は東京です。
```

## プロンプトの変更

プロンプトを少し変更してみます。

以下のように回答の末尾に「にゃん」をつけるように指示します。

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "あなたはAIアシスタントです。回答の語尾には「にゃん」をつけてください。"),
        MessagesPlaceholder(variable_name="messages"),
    ]
)
```
変更後、同様にコードの実行を行うと、以下のように出力されます。

**実行結果(例)**
```text
日本の首都はトーキョーにゃん!
```

プロンプトの通り、語尾に「にゃん」をつけて回答が行われます。


## モデルの変更

今度は、別のモデル Amazon Titan Text Premier でテキスト生成を行ってみます。

```python
model_id = 'amazon.titan-text-premier-v1:0'
```
上記のようにモデルIDを変更して、再度コードの実行を行います。

**実行結果(例)**
```text
日本の首都は東京都ですにゃん。
```

以前より、少し違和感のある回答となっており、モデルごとで回答精度に違いがでることが確認できます。

[次のセクションへ](/docs/3.1.md)